{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "upZK6q0-QWz9",
   "metadata": {
    "id": "upZK6q0-QWz9"
   },
   "outputs": [],
   "source": [
    "#!pip install -q openai gradio anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ts8J2TotLlCv",
   "metadata": {
    "id": "Ts8J2TotLlCv"
   },
   "source": [
    "#### For token generation\n",
    "\n",
    "- OpenAI: https://platform.openai.com/api-keys  \n",
    "- Anthropic: https://console.anthropic.com/settings/keys  \n",
    "- Google: https://ai.google.dev/gemini-api\n",
    "- Huggingface : https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ouXqjh3NOXYb",
   "metadata": {
    "id": "ouXqjh3NOXYb"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "t1ktUFgvL3iL",
   "metadata": {
    "id": "t1ktUFgvL3iL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY','')\n",
    "anthropic_api_key = os.environ.get('ANTHROPIC_API_KEY','')\n",
    "google_api_key = os.environ.get('GOOGLE_API_KEY','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CT0wle24QmZC",
   "metadata": {
    "id": "CT0wle24QmZC"
   },
   "outputs": [],
   "source": [
    "def openai_api_stream_chat(openai_instance,model,history):\n",
    "    stream = openai_instance.chat.completions.create(model=model, messages=history, stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "MusTsqRKOXUe",
   "metadata": {
    "id": "MusTsqRKOXUe"
   },
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=openai_api_key)\n",
    "def openai_stream_chat(model,history):\n",
    "    return openai_api_stream_chat(openai,model,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Ib4GwoQV4Ovb",
   "metadata": {
    "id": "Ib4GwoQV4Ovb"
   },
   "outputs": [],
   "source": [
    "gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "def gemini_stream_chat(model,history):\n",
    "    return openai_api_stream_chat(gemini_client,model,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "YoLBhTVNmMwB",
   "metadata": {
    "id": "YoLBhTVNmMwB"
   },
   "outputs": [],
   "source": [
    "anthropic = Anthropic(api_key=anthropic_api_key)\n",
    "def claude_stream_chat(model, history,system_message):\n",
    "\n",
    "    # Create streaming response\n",
    "    stream = anthropic.messages.create(\n",
    "        model=model,\n",
    "        messages=history,\n",
    "        stream=True,\n",
    "        max_tokens=4096,\n",
    "        system = system_message\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.type == \"content_block_delta\":\n",
    "            response += chunk.delta.text or ''\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85288cfb-9d35-4c92-8679-72ec83df0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def is_not_blank(string):\n",
    "    return bool(re.search(r'\\S', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6Rc7MWn1WdkG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "6Rc7MWn1WdkG",
    "outputId": "398b986d-7952-43e6-98bf-b0e39cb2fc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://4c1502c95175d2aa8c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4c1502c95175d2aa8c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all query will in the context of flutter app development with dart\n",
      "all query will in the context of flutter app development with dart\n",
      "all query will in the context of flutter app development with dart\n",
      "all query will in the context of flutter app development with dart\n",
      "all query will in the context of flutter app development with dart\n",
      "all query will in the context of flutter app development with dart\n"
     ]
    }
   ],
   "source": [
    "model_choices = [\"claude-3-sonnet-20240229\", \"gpt-4o-mini\",\"gpt-4o\", \"gpt-3.5-turbo\",\"gemini-1.5-flash\"]\n",
    "css = \"\"\"\n",
    ".custom-row {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "}\n",
    "footer {display: none !important}\n",
    ".gradio-container {min-height: 0px !important}\n",
    "\"\"\"\n",
    "def call_llm(history, selected_model: str,system_message):\n",
    "    print(system_message)\n",
    "    cleaned_history = []\n",
    "    if system_message and is_not_blank(system_message) and (not 'claude' in selected_model):\n",
    "        cleaned_history.append({\"role\": 'system', \"content\": system_message})\n",
    "    for msg in history:\n",
    "        cleaned_history.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "    if 'gpt' in selected_model:\n",
    "        return openai_stream_chat(selected_model, history=cleaned_history)\n",
    "    elif 'claude' in selected_model:\n",
    "        return claude_stream_chat(selected_model, history=cleaned_history,system_message=system_message)\n",
    "    elif 'gemini' in selected_model:\n",
    "        return gemini_stream_chat(selected_model, history=cleaned_history)\n",
    "\n",
    "with gr.Blocks(css=css,title= 'LLM chatbots') as ui:\n",
    "    selected_model = gr.State(value=model_choices[0])  # Use gr.State to store the current model\n",
    "\n",
    "    dropdown = gr.Dropdown(\n",
    "        label=\"Choose a model\",\n",
    "        choices=model_choices,\n",
    "        interactive=True,\n",
    "        value=model_choices[0]\n",
    "    )\n",
    "    \n",
    "    system_message = gr.Textbox(label=\"System message:\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "    entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def update_model(new_model):\n",
    "        return new_model\n",
    "\n",
    "    def do_entry(message, history, current_model,system_message_text):\n",
    "        history += [{\"role\": \"user\", \"content\": message}]\n",
    "        yield \"\", history\n",
    "        for chunk in call_llm(history, current_model,system_message_text):\n",
    "            yield '', [*history, {\"role\": \"assistant\", \"content\": chunk}]\n",
    "\n",
    "    # Update the model state when dropdown changes\n",
    "    dropdown.change(\n",
    "        update_model,\n",
    "        inputs=[dropdown],\n",
    "        outputs=[selected_model]\n",
    "    )\n",
    "\n",
    "    # Pass the current model state to do_entry\n",
    "    entry.submit(\n",
    "        do_entry,\n",
    "        inputs=[entry, chatbot, selected_model,system_message],\n",
    "        outputs=[entry, chatbot]\n",
    "    )\n",
    "\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True,share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e28660-00fd-48aa-85e7-777ebe657ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
